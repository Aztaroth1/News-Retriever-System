{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2cf67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pandas feedparser nltk beautifulsoup4 requests numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01538f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… punkt ya estÃ¡ disponible\n",
      "âœ… stopwords ya estÃ¡ disponible\n",
      "ðŸ“¥ Descargando wordnet...\n",
      "âœ… wordnet descargado correctamente\n",
      "âœ… averaged_perceptron_tagger ya estÃ¡ disponible\n",
      "âœ… punkt_tab ya estÃ¡ disponible\n",
      "âœ… LibrerÃ­as y recursos NLTK configurados\n"
     ]
    }
   ],
   "source": [
    "# Sistema Inteligente de BÃºsqueda y ClasificaciÃ³n de Noticias\n",
    "# MÃ³dulos: Crawling, Procesamiento e IntegraciÃ³n de Dataset\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import feedparser\n",
    "import json\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tag import pos_tag\n",
    "import string\n",
    "from collections import Counter\n",
    "import time\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Descargar recursos de NLTK necesarios\n",
    "def download_nltk_resources():\n",
    "    \"\"\"Descargar todos los recursos de NLTK necesarios\"\"\"\n",
    "    resources = [\n",
    "        ('tokenizers/punkt', 'punkt'),\n",
    "        ('corpora/stopwords', 'stopwords'),\n",
    "        ('corpora/wordnet', 'wordnet'),\n",
    "        ('taggers/averaged_perceptron_tagger', 'averaged_perceptron_tagger'),\n",
    "        ('tokenizers/punkt_tab', 'punkt_tab')\n",
    "    ]\n",
    "    \n",
    "    for resource_path, resource_name in resources:\n",
    "        try:\n",
    "            nltk.data.find(resource_path)\n",
    "            print(f\"âœ… {resource_name} ya estÃ¡ disponible\")\n",
    "        except LookupError:\n",
    "            print(f\"ðŸ“¥ Descargando {resource_name}...\")\n",
    "            try:\n",
    "                nltk.download(resource_name, quiet=True)\n",
    "                print(f\"âœ… {resource_name} descargado correctamente\")\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ Error descargando {resource_name}: {e}\")\n",
    "\n",
    "# Descargar recursos\n",
    "download_nltk_resources()\n",
    "print(\"âœ… LibrerÃ­as y recursos NLTK configurados\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "755d6cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MÃ“DULO 1: CRAWLING Y RECOPILACIÃ“N DE DOCUMENTOS\n",
    "# =============================================================================\n",
    "\n",
    "class NewsCrawler:\n",
    "    \"\"\"Crawler para recopilar noticias de diferentes fuentes\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "        })\n",
    "        self.crawled_articles = []\n",
    "    \n",
    "    def crawl_rss_feed(self, feed_url, max_articles=20):\n",
    "        \"\"\"Crawl noticias desde RSS feeds\"\"\"\n",
    "        try:\n",
    "            print(f\"ðŸ” Crawling RSS: {feed_url}\")\n",
    "            feed = feedparser.parse(feed_url)\n",
    "            articles = []\n",
    "            \n",
    "            for entry in feed.entries[:max_articles]:\n",
    "                article = {\n",
    "                    'title': getattr(entry, 'title', ''),\n",
    "                    'description': getattr(entry, 'summary', ''),\n",
    "                    'link': getattr(entry, 'link', ''),\n",
    "                    'published': getattr(entry, 'published', ''),\n",
    "                    'source': feed_url,\n",
    "                    'content_type': 'rss'\n",
    "                }\n",
    "                \n",
    "                # Intentar obtener contenido completo\n",
    "                try:\n",
    "                    content = self.extract_article_content(article['link'])\n",
    "                    if content:\n",
    "                        article['full_content'] = content\n",
    "                    else:\n",
    "                        article['full_content'] = article['description']\n",
    "                except:\n",
    "                    article['full_content'] = article['description']\n",
    "                \n",
    "                articles.append(article)\n",
    "                time.sleep(0.5)  # Ser respetuoso con el servidor\n",
    "            \n",
    "            print(f\"âœ… Obtenidos {len(articles)} artÃ­culos de RSS\")\n",
    "            return articles\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error crawling RSS {feed_url}: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def extract_article_content(self, url):\n",
    "        \"\"\"Extraer contenido del artÃ­culo desde URL\"\"\"\n",
    "        try:\n",
    "            response = self.session.get(url, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # Remover scripts y estilos\n",
    "            for script in soup([\"script\", \"style\"]):\n",
    "                script.decompose()\n",
    "            \n",
    "            # Buscar contenido principal\n",
    "            content_selectors = [\n",
    "                'article', '.article-content', '.post-content', \n",
    "                '.entry-content', '.content', 'main', '.story-body'\n",
    "            ]\n",
    "            \n",
    "            content = \"\"\n",
    "            for selector in content_selectors:\n",
    "                elements = soup.select(selector)\n",
    "                if elements:\n",
    "                    content = ' '.join([elem.get_text(strip=True) for elem in elements])\n",
    "                    break\n",
    "            \n",
    "            if not content:\n",
    "                # Fallback: obtener todo el texto\n",
    "                content = soup.get_text(strip=True)\n",
    "            \n",
    "            return content[:5000]  # Limitar longitud\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸  Error extrayendo contenido de {url}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def crawl_news_websites(self, urls, max_articles_per_site=10):\n",
    "        \"\"\"Crawl noticias desde sitios web especÃ­ficos\"\"\"\n",
    "        all_articles = []\n",
    "        \n",
    "        for url in urls:\n",
    "            try:\n",
    "                print(f\"ðŸ” Crawling website: {url}\")\n",
    "                response = self.session.get(url, timeout=10)\n",
    "                response.raise_for_status()\n",
    "                soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                \n",
    "                # Buscar enlaces de artÃ­culos\n",
    "                article_links = []\n",
    "                link_selectors = ['a[href*=\"article\"]', 'a[href*=\"news\"]', 'a[href*=\"/20\"]']\n",
    "                \n",
    "                for selector in link_selectors:\n",
    "                    links = soup.select(selector)\n",
    "                    for link in links:\n",
    "                        href = link.get('href')\n",
    "                        if href:\n",
    "                            if href.startswith('/'):\n",
    "                                href = url.rstrip('/') + href\n",
    "                            article_links.append(href)\n",
    "                \n",
    "                # Procesar enlaces Ãºnicos\n",
    "                unique_links = list(set(article_links))[:max_articles_per_site]\n",
    "                \n",
    "                for link in unique_links:\n",
    "                    try:\n",
    "                        content = self.extract_article_content(link)\n",
    "                        if content and len(content) > 100:\n",
    "                            article = {\n",
    "                                'title': self.extract_title_from_content(content),\n",
    "                                'description': content[:300] + '...',\n",
    "                                'link': link,\n",
    "                                'published': datetime.now().isoformat(),\n",
    "                                'source': url,\n",
    "                                'content_type': 'web',\n",
    "                                'full_content': content\n",
    "                            }\n",
    "                            all_articles.append(article)\n",
    "                    except Exception as e:\n",
    "                        continue\n",
    "                    \n",
    "                    time.sleep(0.5)\n",
    "                \n",
    "                print(f\"âœ… Obtenidos {len([a for a in all_articles if a['source'] == url])} artÃ­culos de {url}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"âŒ Error crawling {url}: {e}\")\n",
    "        \n",
    "        return all_articles\n",
    "    \n",
    "    def extract_title_from_content(self, content):\n",
    "        \"\"\"Extraer tÃ­tulo desde el contenido\"\"\"\n",
    "        sentences = sent_tokenize(content)\n",
    "        if sentences:\n",
    "            return sentences[0][:100]\n",
    "        return \"Sin tÃ­tulo\"\n",
    "    \n",
    "    def save_crawled_data(self, articles, filename='crawled_news.json'):\n",
    "        \"\"\"Guardar datos crawleados\"\"\"\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(articles, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"ðŸ’¾ Datos guardados en {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "536c4a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MÃ“DULO 2: PROCESAMIENTO Y PREPROCESAMIENTO\n",
    "# =============================================================================\n",
    "\n",
    "class TextPreprocessor:\n",
    "    \"\"\"Clase para preprocesamiento de texto\"\"\"\n",
    "    \n",
    "    def __init__(self, language='english'):\n",
    "        self.language = language\n",
    "        self.stop_words = set(stopwords.words(language))\n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        \n",
    "        # AÃ±adir stopwords personalizadas\n",
    "        custom_stopwords = {\n",
    "            'said', 'says', 'say', 'told', 'news', 'report', 'reports',\n",
    "            'according', 'reuters', 'cnn', 'bbc', 'associated', 'press'\n",
    "        }\n",
    "        self.stop_words.update(custom_stopwords)\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Limpieza bÃ¡sica del texto\"\"\"\n",
    "        if not text:\n",
    "            return \"\"\n",
    "        \n",
    "        # Convertir a minÃºsculas\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Remover URLs\n",
    "        text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n",
    "        \n",
    "        # Remover emails\n",
    "        text = re.sub(r'\\S+@\\S+', '', text)\n",
    "        \n",
    "        # Remover nÃºmeros y caracteres especiales\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
    "        \n",
    "        # Remover espacios mÃºltiples\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        \n",
    "        return text.strip()\n",
    "    \n",
    "    def tokenize_text(self, text):\n",
    "        \"\"\"TokenizaciÃ³n del texto\"\"\"\n",
    "        try:\n",
    "            tokens = word_tokenize(text)\n",
    "            return tokens\n",
    "        except:\n",
    "            return text.split()\n",
    "    \n",
    "    def remove_stopwords(self, tokens):\n",
    "        \"\"\"Remover stopwords\"\"\"\n",
    "        return [token for token in tokens if token not in self.stop_words and len(token) > 2]\n",
    "    \n",
    "    def stem_tokens(self, tokens):\n",
    "        \"\"\"Aplicar stemming\"\"\"\n",
    "        return [self.stemmer.stem(token) for token in tokens]\n",
    "    \n",
    "    def lemmatize_tokens(self, tokens):\n",
    "        \"\"\"Aplicar lemmatizaciÃ³n\"\"\"\n",
    "        return [self.lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    def get_pos_tags(self, tokens):\n",
    "        \"\"\"Obtener etiquetas POS\"\"\"\n",
    "        return pos_tag(tokens)\n",
    "    \n",
    "    def filter_by_pos(self, pos_tags, allowed_pos=['NN', 'NNS', 'NNP', 'NNPS', 'JJ', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']):\n",
    "        \"\"\"Filtrar tokens por POS tags\"\"\"\n",
    "        return [word for word, pos in pos_tags if pos in allowed_pos]\n",
    "    \n",
    "    def preprocess_document(self, text, use_stemming=True, use_pos_filtering=True):\n",
    "        \"\"\"Preprocessamiento completo de un documento\"\"\"\n",
    "        # Limpieza\n",
    "        cleaned_text = self.clean_text(text)\n",
    "        \n",
    "        # TokenizaciÃ³n\n",
    "        tokens = self.tokenize_text(cleaned_text)\n",
    "        \n",
    "        # Remover stopwords\n",
    "        tokens = self.remove_stopwords(tokens)\n",
    "        \n",
    "        # Filtrado por POS\n",
    "        if use_pos_filtering:\n",
    "            pos_tags = self.get_pos_tags(tokens)\n",
    "            tokens = self.filter_by_pos(pos_tags)\n",
    "        \n",
    "        # Stemming o LemmatizaciÃ³n\n",
    "        if use_stemming:\n",
    "            tokens = self.stem_tokens(tokens)\n",
    "        else:\n",
    "            tokens = self.lemmatize_tokens(tokens)\n",
    "        \n",
    "        return tokens\n",
    "    \n",
    "    def extract_features(self, text):\n",
    "        \"\"\"Extraer caracterÃ­sticas del texto\"\"\"\n",
    "        tokens = self.preprocess_document(text)\n",
    "        \n",
    "        features = {\n",
    "            'word_count': len(text.split()),\n",
    "            'char_count': len(text),\n",
    "            'sentence_count': len(sent_tokenize(text)),\n",
    "            'avg_word_length': np.mean([len(word) for word in tokens]) if tokens else 0,\n",
    "            'vocabulary_richness': len(set(tokens)) / len(tokens) if tokens else 0,\n",
    "            'most_common_words': Counter(tokens).most_common(10)\n",
    "        }\n",
    "        \n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f4a4ad4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MÃ“DULO 3: INTEGRACIÃ“N DEL DATASET\n",
    "# =============================================================================\n",
    "\n",
    "class DatasetIntegrator:\n",
    "    \"\"\"Clase para integrar y procesar datasets\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.preprocessor = TextPreprocessor()\n",
    "        self.integrated_dataset = None\n",
    "    \n",
    "    def load_kaggle_dataset(self, filepath):\n",
    "        \"\"\"Cargar dataset de Kaggle\"\"\"\n",
    "        try:\n",
    "            # Intentar diferentes separadores\n",
    "            for sep in [',', '\\t', ';']:\n",
    "                try:\n",
    "                    df = pd.read_csv(filepath, sep=sep)\n",
    "                    if len(df.columns) > 1:\n",
    "                        break\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            print(f\"âœ… Dataset de Kaggle cargado: {df.shape[0]} filas, {df.shape[1]} columnas\")\n",
    "            print(f\"Columnas: {list(df.columns)}\")\n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error cargando dataset de Kaggle: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def process_kaggle_dataset(self, df):\n",
    "        \"\"\"Procesar dataset de Kaggle\"\"\"\n",
    "        processed_articles = []\n",
    "        \n",
    "        # Detectar columnas relevantes\n",
    "        text_columns = []\n",
    "        category_column = None\n",
    "        \n",
    "        for col in df.columns:\n",
    "            col_lower = col.lower()\n",
    "            if any(keyword in col_lower for keyword in ['headline', 'title', 'description', 'text', 'content']):\n",
    "                text_columns.append(col)\n",
    "            elif any(keyword in col_lower for keyword in ['category', 'class', 'label', 'topic']):\n",
    "                category_column = col\n",
    "        \n",
    "        print(f\"ðŸ“ Columnas de texto detectadas: {text_columns}\")\n",
    "        print(f\"ðŸ“‚ Columna de categorÃ­a detectada: {category_column}\")\n",
    "        \n",
    "        for idx, row in df.iterrows():\n",
    "            # Combinar texto de todas las columnas relevantes\n",
    "            text_content = \"\"\n",
    "            for col in text_columns:\n",
    "                if pd.notna(row[col]):\n",
    "                    text_content += str(row[col]) + \" \"\n",
    "            \n",
    "            if text_content.strip():\n",
    "                article = {\n",
    "                    'title': str(row[text_columns[0]]) if text_columns else f\"Article {idx}\",\n",
    "                    'description': text_content[:300] + \"...\",\n",
    "                    'full_content': text_content,\n",
    "                    'category': str(row[category_column]) if category_column and pd.notna(row[category_column]) else 'Unknown',\n",
    "                    'source': 'kaggle_dataset',\n",
    "                    'content_type': 'dataset',\n",
    "                    'published': datetime.now().isoformat(),\n",
    "                    'link': f\"kaggle_article_{idx}\"\n",
    "                }\n",
    "                processed_articles.append(article)\n",
    "        \n",
    "        print(f\"âœ… Procesados {len(processed_articles)} artÃ­culos del dataset de Kaggle\")\n",
    "        return processed_articles\n",
    "    \n",
    "    def integrate_datasets(self, crawled_articles=None, kaggle_articles=None):\n",
    "        \"\"\"Integrar todos los datasets\"\"\"\n",
    "        all_articles = []\n",
    "        \n",
    "        if crawled_articles:\n",
    "            all_articles.extend(crawled_articles)\n",
    "            print(f\"âž• AÃ±adidos {len(crawled_articles)} artÃ­culos crawleados\")\n",
    "        \n",
    "        if kaggle_articles:\n",
    "            all_articles.extend(kaggle_articles)\n",
    "            print(f\"âž• AÃ±adidos {len(kaggle_articles)} artÃ­culos del dataset\")\n",
    "        \n",
    "        # Crear DataFrame integrado\n",
    "        df_integrated = pd.DataFrame(all_articles)\n",
    "        \n",
    "        # Limpieza y normalizaciÃ³n\n",
    "        df_integrated = df_integrated.drop_duplicates(subset=['title'], keep='first')\n",
    "        df_integrated = df_integrated.dropna(subset=['full_content'])\n",
    "        df_integrated['processed_at'] = datetime.now().isoformat()\n",
    "        \n",
    "        print(f\"ðŸ”„ Dataset integrado: {len(df_integrated)} artÃ­culos Ãºnicos\")\n",
    "        \n",
    "        self.integrated_dataset = df_integrated\n",
    "        return df_integrated\n",
    "    \n",
    "    def preprocess_integrated_dataset(self, df):\n",
    "        \"\"\"Preprocesar dataset integrado con manejo de errores\"\"\"\n",
    "        print(\"ðŸ”§ Iniciando preprocesamiento...\")\n",
    "        \n",
    "        # Preprocesar textos con manejo de errores\n",
    "        processed_tokens = []\n",
    "        text_features = []\n",
    "        \n",
    "        for idx, content in enumerate(df['full_content']):\n",
    "            try:\n",
    "                tokens = self.preprocessor.preprocess_document(content)\n",
    "                features = self.preprocessor.extract_features(content)\n",
    "                processed_tokens.append(tokens)\n",
    "                text_features.append(features)\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ Error procesando artÃ­culo {idx}: {e}\")\n",
    "                # Fallback bÃ¡sico\n",
    "                basic_tokens = content.lower().split()[:50]  # Primeras 50 palabras\n",
    "                basic_features = {\n",
    "                    'word_count': len(content.split()),\n",
    "                    'char_count': len(content),\n",
    "                    'sentence_count': 1,\n",
    "                    'avg_word_length': 5,\n",
    "                    'vocabulary_richness': 0.5,\n",
    "                    'most_common_words': []\n",
    "                }\n",
    "                processed_tokens.append(basic_tokens)\n",
    "                text_features.append(basic_features)\n",
    "        \n",
    "        df['processed_tokens'] = processed_tokens\n",
    "        df['text_features'] = text_features\n",
    "        \n",
    "        # Crear campos adicionales\n",
    "        df['word_count'] = df['text_features'].apply(lambda x: x['word_count'])\n",
    "        df['sentence_count'] = df['text_features'].apply(lambda x: x['sentence_count'])\n",
    "        df['vocab_richness'] = df['text_features'].apply(lambda x: x['vocabulary_richness'])\n",
    "        \n",
    "        # Limpiar categorÃ­as\n",
    "        if 'category' in df.columns:\n",
    "            df['category'] = df['category'].str.lower().str.strip()\n",
    "            df['category'] = df['category'].fillna('unknown')\n",
    "        \n",
    "        print(\"âœ… Preprocesamiento completado\")\n",
    "        return df\n",
    "    \n",
    "    def get_dataset_statistics(self, df):\n",
    "        \"\"\"Obtener estadÃ­sticas del dataset\"\"\"\n",
    "        stats = {\n",
    "            'total_articles': len(df),\n",
    "            'unique_sources': df['source'].nunique(),\n",
    "            'content_types': df['content_type'].value_counts().to_dict(),\n",
    "            'avg_word_count': df['word_count'].mean(),\n",
    "            'avg_sentence_count': df['sentence_count'].mean(),\n",
    "            'categories': df['category'].value_counts().head(10).to_dict() if 'category' in df.columns else {},\n",
    "            'date_range': {\n",
    "                'earliest': df['published'].min() if 'published' in df.columns else None,\n",
    "                'latest': df['published'].max() if 'published' in df.columns else None\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return stats\n",
    "    \n",
    "    def save_integrated_dataset(self, df, filename='integrated_news_dataset.csv'):\n",
    "        \"\"\"Guardar dataset integrado\"\"\"\n",
    "        # Preparar para guardar (convertir listas a strings)\n",
    "        df_save = df.copy()\n",
    "        df_save['processed_tokens'] = df_save['processed_tokens'].apply(lambda x: ' '.join(x) if isinstance(x, list) else str(x))\n",
    "        df_save['text_features'] = df_save['text_features'].apply(lambda x: json.dumps(x) if isinstance(x, dict) else str(x))\n",
    "        \n",
    "        df_save.to_csv(filename, index=False, encoding='utf-8')\n",
    "        print(f\"ðŸ’¾ Dataset integrado guardado en {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2aa582ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Iniciando Sistema de Crawling y Procesamiento de Noticias\n",
      "\n",
      "============================================================\n",
      "MÃ“DULO 1: CRAWLING Y RECOPILACIÃ“N DE DOCUMENTOS\n",
      "============================================================\n",
      "ðŸ” Intentando crawlear RSS feeds...\n",
      "ðŸ” Crawling RSS: http://rss.cnn.com/rss/edition.rss\n",
      "âš ï¸  Error extrayendo contenido de https://www.cnn.com/collections/intl-trump-040223/: 404 Client Error: Not Found for url: https://edition.cnn.com/collections/intl-trump-040223/\n",
      "âœ… Obtenidos 3 artÃ­culos de RSS\n",
      "âœ… 3 artÃ­culos obtenidos de http://rss.cnn.com/rss/edition.rss\n",
      "ðŸ” Crawling RSS: http://feeds.bbci.co.uk/news/rss.xml\n",
      "âœ… Obtenidos 3 artÃ­culos de RSS\n",
      "âœ… 3 artÃ­culos obtenidos de http://feeds.bbci.co.uk/news/rss.xml\n",
      "ðŸ” Crawling RSS: https://rss.reuters.com/news\n",
      "âœ… Obtenidos 0 artÃ­culos de RSS\n",
      "âš ï¸ No se pudieron obtener artÃ­culos de https://rss.reuters.com/news\n",
      "ðŸ” Crawling RSS: https://feeds.npr.org/1001/rss.xml\n",
      "âœ… Obtenidos 3 artÃ­culos de RSS\n",
      "âœ… 3 artÃ­culos obtenidos de https://feeds.npr.org/1001/rss.xml\n",
      "ðŸ” Crawling RSS: https://rss.dw.com/rdf/rss-en-all\n",
      "âœ… Obtenidos 3 artÃ­culos de RSS\n",
      "âœ… 3 artÃ­culos obtenidos de https://rss.dw.com/rdf/rss-en-all\n",
      "\n",
      "ðŸŒ Intentando crawlear sitios web...\n",
      "ðŸ” Crawling website: https://news.ycombinator.com\n",
      "âš ï¸  Error extrayendo contenido de hide?id=44775830&goto=news: Invalid URL 'hide?id=44775830&goto=news': No scheme supplied. Perhaps you meant https://hide?id=44775830&goto=news?\n",
      "âš ï¸  Error extrayendo contenido de vote?id=44780353&how=up&goto=news: Invalid URL 'vote?id=44780353&how=up&goto=news': No scheme supplied. Perhaps you meant https://vote?id=44780353&how=up&goto=news?\n",
      "âœ… Obtenidos 0 artÃ­culos de https://news.ycombinator.com\n",
      "ðŸ” Crawling website: https://www.reddit.com/r/worldnews/.rss\n",
      "âœ… Obtenidos 0 artÃ­culos de https://www.reddit.com/r/worldnews/.rss\n",
      "ðŸ” Crawling website: https://techcrunch.com\n",
      "âœ… Obtenidos 2 artÃ­culos de https://techcrunch.com\n",
      "âœ… 2 artÃ­culos obtenidos de sitios web\n",
      "\n",
      "ðŸ“Š Total de artÃ­culos crawleados: 14\n",
      "\n",
      "ðŸ“° Ejemplos de artÃ­culos crawleados:\n",
      "\n",
      "--- ArtÃ­culo Crawleado 1 ---\n",
      "TÃ­tulo: Trump pleads not guilty to 34 felony counts...\n",
      "Fuente: http://rss.cnn.com/rss/edition.rss\n",
      "Tipo: rss\n",
      "DescripciÃ³n: ...\n",
      "\n",
      "--- ArtÃ­culo Crawleado 2 ---\n",
      "TÃ­tulo: Haberman reveals why Trump attacked judge and his family in speech...\n",
      "Fuente: http://rss.cnn.com/rss/edition.rss\n",
      "Tipo: rss\n",
      "DescripciÃ³n: CNN political contributor Maggie Haberman explains the reasoning behind Donald Trump's attacks on th...\n",
      "\n",
      "============================================================\n",
      "MÃ“DULO 2: CARGA DEL DATASET DE KAGGLE\n",
      "============================================================\n",
      "ðŸ“ Creando dataset de ejemplo para demostraciÃ³n...\n",
      "ðŸ“ Columnas de texto detectadas: ['headline', 'short_description']\n",
      "ðŸ“‚ Columna de categorÃ­a detectada: category\n",
      "âœ… Procesados 10 artÃ­culos del dataset de Kaggle\n",
      "\n",
      "============================================================\n",
      "MÃ“DULO 3: INTEGRACIÃ“N DEL DATASET\n",
      "============================================================\n",
      "âž• AÃ±adidos 14 artÃ­culos crawleados\n",
      "âž• AÃ±adidos 10 artÃ­culos del dataset\n",
      "ðŸ”„ Dataset integrado: 24 artÃ­culos Ãºnicos\n",
      "ðŸ”§ Iniciando preprocesamiento...\n",
      "âš ï¸ Error procesando artÃ­culo 0: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('averaged_perceptron_tagger_eng')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\DELL/nltk_data'\n",
      "    - 'c:\\\\Users\\\\DELL\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\nltk_data'\n",
      "    - 'c:\\\\Users\\\\DELL\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\share\\\\nltk_data'\n",
      "    - 'c:\\\\Users\\\\DELL\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\DELL\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "âš ï¸ Error procesando artÃ­culo 1: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('averaged_perceptron_tagger_eng')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\DELL/nltk_data'\n",
      "    - 'c:\\\\Users\\\\DELL\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\nltk_data'\n",
      "    - 'c:\\\\Users\\\\DELL\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\share\\\\nltk_data'\n",
      "    - 'c:\\\\Users\\\\DELL\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\DELL\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "âš ï¸ Error procesando artÃ­culo 2: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('averaged_perceptron_tagger_eng')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\DELL/nltk_data'\n",
      "    - 'c:\\\\Users\\\\DELL\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\nltk_data'\n",
      "    - 'c:\\\\Users\\\\DELL\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\share\\\\nltk_data'\n",
      "    - 'c:\\\\Users\\\\DELL\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\DELL\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "âš ï¸ Error procesando artÃ­culo 3: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('averaged_perceptron_tagger_eng')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\DELL/nltk_data'\n",
      "    - 'c:\\\\Users\\\\DELL\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\nltk_data'\n",
      "    - 'c:\\\\Users\\\\DELL\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\share\\\\nltk_data'\n",
      "    - 'c:\\\\Users\\\\DELL\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\DELL\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "âš ï¸ Error procesando artÃ­culo 4: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('averaged_perceptron_tagger_eng')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\DELL/nltk_data'\n",
      "    - 'c:\\\\Users\\\\DELL\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\nltk_data'\n",
      "    - 'c:\\\\Users\\\\DELL\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\share\\\\nltk_data'\n",
      "    - 'c:\\\\Users\\\\DELL\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\DELL\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "âš ï¸ Error procesando artÃ­culo 5: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('averaged_perceptron_tagger_eng')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\DELL/nltk_data'\n",
      "    - 'c:\\\\Users\\\\DELL\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\nltk_data'\n",
      "    - 'c:\\\\Users\\\\DELL\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\share\\\\nltk_data'\n",
      "    - 'c:\\\\Users\\\\DELL\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\DELL\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "âš ï¸ Error procesando artÃ­culo 6: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('averaged_perceptron_tagger_eng')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\DELL/nltk_data'\n",
      "    - 'c:\\\\Users\\\\DELL\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\nltk_data'\n",
      "    - 'c:\\\\Users\\\\DELL\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\share\\\\nltk_data'\n",
      "    - 'c:\\\\Users\\\\DELL\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\DELL\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "âš ï¸ Error procesando artÃ­culo 7: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('averaged_perceptron_tagger_eng')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\DELL/nltk_data'\n",
      "    - 'c:\\\\Users\\\\DELL\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\nltk_data'\n",
      "    - 'c:\\\\Users\\\\DELL\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\share\\\\nltk_data'\n",
      "    - 'c:\\\\Users\\\\DELL\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\DELL\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "âš ï¸ Error procesando artÃ­culo 8: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('averaged_perceptron_tagger_eng')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\DELL/nltk_data'\n",
      "    - 'c:\\\\Users\\\\DELL\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\nltk_data'\n",
      "    - 'c:\\\\Users\\\\DELL\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\share\\\\nltk_data'\n",
      "    - 'c:\\\\Users\\\\DELL\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\DELL\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "âš ï¸ Error procesando artÃ­culo 9: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('averaged_perceptron_tagger_eng')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\DELL/nltk_data'\n",
      "    - 'c:\\\\Users\\\\DELL\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\nltk_data'\n",
      "    - 'c:\\\\Users\\\\DELL\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\share\\\\nltk_data'\n",
      "    - 'c:\\\\Users\\\\DELL\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\DELL\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "âš ï¸ Error procesando artÃ­culo 10: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('averaged_perceptron_tagger_eng')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\DELL/nltk_data'\n",
      "    - 'c:\\\\Users\\\\DELL\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\nltk_data'\n",
      "    - 'c:\\\\Users\\\\DELL\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\share\\\\nltk_data'\n",
      "    - 'c:\\\\Users\\\\DELL\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\DELL\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "âš ï¸ Error procesando artÃ­culo 11: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('averaged_perceptron_tagger_eng')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\DELL/nltk_data'\n",
      "    - 'c:\\\\Users\\\\DELL\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\nltk_data'\n",
      "    - 'c:\\\\Users\\\\DELL\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\share\\\\nltk_data'\n",
      "    - 'c:\\\\Users\\\\DELL\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\DELL\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "âš ï¸ Error procesando artÃ­culo 12: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('averaged_perceptron_tagger_eng')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\DELL/nltk_data'\n",
      "    - 'c:\\\\Users\\\\DELL\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\nltk_data'\n",
      "    - 'c:\\\\Users\\\\DELL\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\share\\\\nltk_data'\n",
      "    - 'c:\\\\Users\\\\DELL\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\DELL\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "âš ï¸ Error procesando artÃ­culo 13: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('averaged_perceptron_tagger_eng')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\DELL/nltk_data'\n",
      "    - 'c:\\\\Users\\\\DELL\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\nltk_data'\n",
      "    - 'c:\\\\Users\\\\DELL\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\share\\\\nltk_data'\n",
      "    - 'c:\\\\Users\\\\DELL\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\DELL\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "âš ï¸ Error procesando artÃ­culo 14: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('averaged_perceptron_tagger_eng')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\DELL/nltk_data'\n",
      "    - 'c:\\\\Users\\\\DELL\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\nltk_data'\n",
      "    - 'c:\\\\Users\\\\DELL\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\share\\\\nltk_data'\n",
      "    - 'c:\\\\Users\\\\DELL\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\DELL\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "âš ï¸ Error procesando artÃ­culo 15: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('averaged_perceptron_tagger_eng')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\DELL/nltk_data'\n",
      "    - 'c:\\\\Users\\\\DELL\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\nltk_data'\n",
      "    - 'c:\\\\Users\\\\DELL\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\share\\\\nltk_data'\n",
      "    - 'c:\\\\Users\\\\DELL\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\DELL\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "âš ï¸ Error procesando artÃ­culo 16: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('averaged_perceptron_tagger_eng')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\DELL/nltk_data'\n",
      "    - 'c:\\\\Users\\\\DELL\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\nltk_data'\n",
      "    - 'c:\\\\Users\\\\DELL\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\share\\\\nltk_data'\n",
      "    - 'c:\\\\Users\\\\DELL\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\DELL\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "âš ï¸ Error procesando artÃ­culo 17: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('averaged_perceptron_tagger_eng')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\DELL/nltk_data'\n",
      "    - 'c:\\\\Users\\\\DELL\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\nltk_data'\n",
      "    - 'c:\\\\Users\\\\DELL\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\share\\\\nltk_data'\n",
      "    - 'c:\\\\Users\\\\DELL\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\DELL\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "âš ï¸ Error procesando artÃ­culo 18: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('averaged_perceptron_tagger_eng')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\DELL/nltk_data'\n",
      "    - 'c:\\\\Users\\\\DELL\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\nltk_data'\n",
      "    - 'c:\\\\Users\\\\DELL\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\share\\\\nltk_data'\n",
      "    - 'c:\\\\Users\\\\DELL\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\DELL\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "âš ï¸ Error procesando artÃ­culo 19: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('averaged_perceptron_tagger_eng')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\DELL/nltk_data'\n",
      "    - 'c:\\\\Users\\\\DELL\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\nltk_data'\n",
      "    - 'c:\\\\Users\\\\DELL\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\share\\\\nltk_data'\n",
      "    - 'c:\\\\Users\\\\DELL\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\DELL\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "âš ï¸ Error procesando artÃ­culo 20: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('averaged_perceptron_tagger_eng')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\DELL/nltk_data'\n",
      "    - 'c:\\\\Users\\\\DELL\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\nltk_data'\n",
      "    - 'c:\\\\Users\\\\DELL\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\share\\\\nltk_data'\n",
      "    - 'c:\\\\Users\\\\DELL\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\DELL\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "âš ï¸ Error procesando artÃ­culo 21: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('averaged_perceptron_tagger_eng')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\DELL/nltk_data'\n",
      "    - 'c:\\\\Users\\\\DELL\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\nltk_data'\n",
      "    - 'c:\\\\Users\\\\DELL\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\share\\\\nltk_data'\n",
      "    - 'c:\\\\Users\\\\DELL\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\DELL\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "âš ï¸ Error procesando artÃ­culo 22: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('averaged_perceptron_tagger_eng')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\DELL/nltk_data'\n",
      "    - 'c:\\\\Users\\\\DELL\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\nltk_data'\n",
      "    - 'c:\\\\Users\\\\DELL\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\share\\\\nltk_data'\n",
      "    - 'c:\\\\Users\\\\DELL\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\DELL\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "âš ï¸ Error procesando artÃ­culo 23: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('averaged_perceptron_tagger_eng')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\DELL/nltk_data'\n",
      "    - 'c:\\\\Users\\\\DELL\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\nltk_data'\n",
      "    - 'c:\\\\Users\\\\DELL\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\share\\\\nltk_data'\n",
      "    - 'c:\\\\Users\\\\DELL\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\DELL\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "âœ… Preprocesamiento completado\n",
      "\n",
      "============================================================\n",
      "RESULTADOS Y ESTADÃSTICAS\n",
      "============================================================\n",
      "ðŸ“Š Total de artÃ­culos procesados: 24\n",
      "ðŸŒ Fuentes Ãºnicas: 6\n",
      "ðŸ“ Promedio de palabras por artÃ­culo: 334.8\n",
      "ðŸ“„ Promedio de oraciones por artÃ­culo: 1.0\n",
      "\n",
      "ðŸ“‚ DistribuciÃ³n por tipo de contenido:\n",
      "  - rss: 12 artÃ­culos\n",
      "  - dataset: 10 artÃ­culos\n",
      "  - web: 2 artÃ­culos\n",
      "\n",
      "ðŸ·ï¸  Top categorÃ­as:\n",
      "  - unknown: 14 artÃ­culos\n",
      "  - technology: 2 artÃ­culos\n",
      "  - environment: 2 artÃ­culos\n",
      "  - business: 2 artÃ­culos\n",
      "  - science: 2 artÃ­culos\n",
      "\n",
      "ðŸ“° Ejemplos de artÃ­culos procesados:\n",
      "\n",
      "--- ArtÃ­culo Procesado 1 ---\n",
      "TÃ­tulo: Trump pleads not guilty to 34 felony counts...\n",
      "CategorÃ­a: unknown\n",
      "Fuente: http://rss.cnn.com/rss/edition.rss\n",
      "Tipo de contenido: rss\n",
      "Palabras: 743\n",
      "Tokens procesados: what we covered hereformer presidentdonald trumparrived in new york city...\n",
      "\n",
      "--- ArtÃ­culo Procesado 2 ---\n",
      "TÃ­tulo: Haberman reveals why Trump attacked judge and his family in speech...\n",
      "CategorÃ­a: unknown\n",
      "Fuente: http://rss.cnn.com/rss/edition.rss\n",
      "Tipo de contenido: rss\n",
      "Palabras: 509\n",
      "Tokens procesados: haberman reveals why trump attacked judge and his family in...\n",
      "\n",
      "--- ArtÃ­culo Procesado 3 ---\n",
      "TÃ­tulo: What to know about the Trump indictment on the eve of his court appearance...\n",
      "CategorÃ­a: unknown\n",
      "Fuente: http://rss.cnn.com/rss/edition.rss\n",
      "Tipo de contenido: rss\n",
      "Palabras: 0\n",
      "Tokens procesados: ...\n",
      "\n",
      "ðŸ” Resumen de fuentes:\n",
      "  - ArtÃ­culos crawleados: 14\n",
      "  - ArtÃ­culos del dataset: 10\n",
      "ðŸ’¾ Dataset integrado guardado en integrated_news_dataset.csv\n",
      "ðŸ’¾ Datos guardados en crawled_news.json\n",
      "\n",
      "âœ… Â¡Pipeline completado exitosamente!\n",
      "\n",
      "Archivos generados:\n",
      "- integrated_news_dataset.csv (Dataset completo procesado)\n",
      "- crawled_news.json (Datos crawleados en formato JSON)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# EJECUCIÃ“N PRINCIPAL\n",
    "# =============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"FunciÃ³n principal que ejecuta todo el pipeline\"\"\"\n",
    "    print(\"ðŸš€ Iniciando Sistema de Crawling y Procesamiento de Noticias\\n\")\n",
    "    \n",
    "    # Inicializar componentes\n",
    "    crawler = NewsCrawler()\n",
    "    integrator = DatasetIntegrator()\n",
    "    \n",
    "    # 1. CRAWLING DE NOTICIAS\n",
    "    print(\"=\" * 60)\n",
    "    print(\"MÃ“DULO 1: CRAWLING Y RECOPILACIÃ“N DE DOCUMENTOS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    crawled_articles = []\n",
    "    \n",
    "    # RSS Feeds populares\n",
    "    rss_feeds = [\n",
    "        'http://rss.cnn.com/rss/edition.rss',\n",
    "        'http://feeds.bbci.co.uk/news/rss.xml',\n",
    "        'https://rss.reuters.com/news',\n",
    "        'https://feeds.npr.org/1001/rss.xml',\n",
    "        'https://rss.dw.com/rdf/rss-en-all'\n",
    "    ]\n",
    "    \n",
    "    print(\"ðŸ” Intentando crawlear RSS feeds...\")\n",
    "    for feed in rss_feeds:\n",
    "        try:\n",
    "            articles = crawler.crawl_rss_feed(feed, max_articles=3)\n",
    "            if articles:\n",
    "                crawled_articles.extend(articles)\n",
    "                print(f\"âœ… {len(articles)} artÃ­culos obtenidos de {feed}\")\n",
    "            else:\n",
    "                print(f\"âš ï¸ No se pudieron obtener artÃ­culos de {feed}\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error con {feed}: {e}\")\n",
    "    \n",
    "    # Sitios web alternativos\n",
    "    web_sources = [\n",
    "        'https://news.ycombinator.com',\n",
    "        'https://www.reddit.com/r/worldnews/.rss',\n",
    "        'https://techcrunch.com'\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nðŸŒ Intentando crawlear sitios web...\")\n",
    "    try:\n",
    "        web_articles = crawler.crawl_news_websites(web_sources, max_articles_per_site=2)\n",
    "        if web_articles:\n",
    "            crawled_articles.extend(web_articles)\n",
    "            print(f\"âœ… {len(web_articles)} artÃ­culos obtenidos de sitios web\")\n",
    "        else:\n",
    "            print(\"âš ï¸ No se pudieron obtener artÃ­culos de sitios web\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error crawleando sitios web: {e}\")\n",
    "    \n",
    "    # Si no se pudieron obtener artÃ­culos reales, usar ejemplos\n",
    "    if not crawled_articles:\n",
    "        print(\"\\nðŸ“ No se pudieron obtener artÃ­culos reales, usando ejemplos de demostraciÃ³n...\")\n",
    "        \n",
    "        crawled_articles = [\n",
    "            {\n",
    "                'title': 'Tech Innovation Drives Market Growth This Quarter',\n",
    "                'description': 'Technology sector shows remarkable performance this quarter with AI breakthroughs...',\n",
    "                'full_content': 'Technology sector shows remarkable performance this quarter with significant innovations in artificial intelligence and machine learning. Companies are investing heavily in research and development to stay competitive in the rapidly evolving digital landscape. Major tech giants report record-breaking revenues driven by cloud computing services and AI-powered solutions. The semiconductor industry continues to expand despite global supply chain challenges, with new manufacturing facilities being established worldwide.',\n",
    "                'source': 'example_tech_news',\n",
    "                'content_type': 'web',\n",
    "                'published': '2025-01-15T10:30:00',\n",
    "                'link': 'https://example.com/tech-growth'\n",
    "            },\n",
    "            {\n",
    "                'title': 'Environmental Policies Show Measurable Positive Impact',\n",
    "                'description': 'New environmental regulations are yielding measurable results across sectors...',\n",
    "                'full_content': 'New environmental regulations are yielding measurable results across multiple sectors. Air quality improvements have been documented in major cities, while renewable energy adoption continues to accelerate nationwide. Solar and wind power installations have reached new records, contributing to a significant reduction in carbon emissions. Government incentives for clean energy have spurred innovation in battery technology and energy storage solutions.',\n",
    "                'source': 'example_env_news',\n",
    "                'content_type': 'rss',\n",
    "                'published': '2025-01-14T15:45:00',\n",
    "                'link': 'https://example.com/env-impact'\n",
    "            },\n",
    "            {\n",
    "                'title': 'Global Economic Indicators Point to Steady Recovery',\n",
    "                'description': 'International markets show signs of stabilization and growth...',\n",
    "                'full_content': 'International markets show signs of stabilization and growth following recent economic uncertainties. Employment rates have improved across developed nations, with particular strength in technology and healthcare sectors. Consumer confidence indices have reached pre-pandemic levels in several major economies. Central banks maintain cautious optimism while monitoring inflation trends and adjusting monetary policies accordingly.',\n",
    "                'source': 'example_business_news',\n",
    "                'content_type': 'web',\n",
    "                'published': '2025-01-13T09:20:00',\n",
    "                'link': 'https://example.com/economic-recovery'\n",
    "            }\n",
    "        ]\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Total de artÃ­culos crawleados: {len(crawled_articles)}\")\n",
    "    \n",
    "    # Mostrar algunos ejemplos de artÃ­culos crawleados\n",
    "    if crawled_articles:\n",
    "        print(\"\\nðŸ“° Ejemplos de artÃ­culos crawleados:\")\n",
    "        for i, article in enumerate(crawled_articles[:2]):\n",
    "            print(f\"\\n--- ArtÃ­culo Crawleado {i+1} ---\")\n",
    "            print(f\"TÃ­tulo: {article['title'][:70]}...\")\n",
    "            print(f\"Fuente: {article['source']}\")\n",
    "            print(f\"Tipo: {article['content_type']}\")\n",
    "            print(f\"DescripciÃ³n: {article['description'][:100]}...\")\n",
    "    \n",
    "    # 2. CARGA DEL DATASET DE KAGGLE\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"MÃ“DULO 2: CARGA DEL DATASET DE KAGGLE\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Crear un dataset de ejemplo mÃ¡s robusto\n",
    "    print(\"ðŸ“ Creando dataset de ejemplo para demostraciÃ³n...\")\n",
    "    \n",
    "    example_data = {\n",
    "        'headline': [\n",
    "            'Breaking: New AI Technology Revolutionizes Healthcare Diagnosis',\n",
    "            'Climate Change Effects Visible in Arctic Region Show Alarming Trends',\n",
    "            'Stock Market Reaches New Record High Driven by Tech Sector',\n",
    "            'Major Breakthrough in Renewable Energy Research Promises Cost Reduction',\n",
    "            'International Trade Agreements Under Review by Multiple Nations',\n",
    "            'Education Reform Bill Passes Senate Vote with Bipartisan Support',\n",
    "            'Tech Giant Announces New Product Launch with Revolutionary Features',\n",
    "            'Environmental Protection Measures Implemented Across National Parks',\n",
    "            'Economic Recovery Shows Positive Signs in Manufacturing Sector',\n",
    "            'Scientific Discovery Changes Understanding of Quantum Physics'\n",
    "        ],\n",
    "        'category': [\n",
    "            'TECHNOLOGY', 'ENVIRONMENT', 'BUSINESS', 'SCIENCE', 'POLITICS',\n",
    "            'EDUCATION', 'TECHNOLOGY', 'ENVIRONMENT', 'BUSINESS', 'SCIENCE'\n",
    "        ],\n",
    "        'short_description': [\n",
    "            'Revolutionary AI system promises to transform patient care and medical diagnosis through advanced machine learning algorithms that can detect diseases earlier than traditional methods.',\n",
    "            'Scientists document rapid changes in Arctic ice patterns and wildlife behavior due to global warming trends, with implications for sea level rise and climate stability.',\n",
    "            'Markets surge to unprecedented levels driven by technology sector growth and investor confidence in artificial intelligence and cloud computing companies.',\n",
    "            'Researchers develop highly efficient solar panel technology breakthrough that could revolutionize clean energy production and reduce costs by up to 40 percent.',\n",
    "            'Government officials review existing trade partnerships and agreements to improve economic relationships and address supply chain disruptions affecting global commerce.',\n",
    "            'New legislation aims to modernize educational standards nationwide and improve student outcomes through increased funding and curriculum updates.',\n",
    "            'Company unveils innovative consumer technology product line featuring cutting-edge features like advanced AI integration and sustainable materials.',\n",
    "            'New policies protect endangered species and natural habitats through comprehensive conservation efforts and increased enforcement of environmental regulations.',\n",
    "            'Economic indicators suggest sustained growth and job creation across multiple industry sectors, with manufacturing showing particular strength.',\n",
    "            'Quantum physics research reveals new fundamental particles that challenge existing scientific theories and could lead to breakthrough technologies.'\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    df_kaggle = pd.DataFrame(example_data)\n",
    "    kaggle_articles = integrator.process_kaggle_dataset(df_kaggle)\n",
    "    \n",
    "    # 3. INTEGRACIÃ“N Y PROCESAMIENTO\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"MÃ“DULO 3: INTEGRACIÃ“N DEL DATASET\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Integrar datasets\n",
    "    df_integrated = integrator.integrate_datasets(crawled_articles, kaggle_articles)\n",
    "    \n",
    "    # Preprocesar dataset integrado\n",
    "    df_processed = integrator.preprocess_integrated_dataset(df_integrated)\n",
    "    \n",
    "    # Obtener estadÃ­sticas\n",
    "    stats = integrator.get_dataset_statistics(df_processed)\n",
    "    \n",
    "    # 4. MOSTRAR RESULTADOS\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"RESULTADOS Y ESTADÃSTICAS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(f\"ðŸ“Š Total de artÃ­culos procesados: {stats['total_articles']}\")\n",
    "    print(f\"ðŸŒ Fuentes Ãºnicas: {stats['unique_sources']}\")\n",
    "    print(f\"ðŸ“ Promedio de palabras por artÃ­culo: {stats['avg_word_count']:.1f}\")\n",
    "    print(f\"ðŸ“„ Promedio de oraciones por artÃ­culo: {stats['avg_sentence_count']:.1f}\")\n",
    "    \n",
    "    print(\"\\nðŸ“‚ DistribuciÃ³n por tipo de contenido:\")\n",
    "    for content_type, count in stats['content_types'].items():\n",
    "        print(f\"  - {content_type}: {count} artÃ­culos\")\n",
    "    \n",
    "    print(\"\\nðŸ·ï¸  Top categorÃ­as:\")\n",
    "    for category, count in list(stats['categories'].items())[:5]:\n",
    "        print(f\"  - {category}: {count} artÃ­culos\")\n",
    "    \n",
    "    # Mostrar ejemplos de artÃ­culos procesados\n",
    "    print(\"\\nðŸ“° Ejemplos de artÃ­culos procesados:\")\n",
    "    for i, (_, article) in enumerate(df_processed.head(3).iterrows()):\n",
    "        print(f\"\\n--- ArtÃ­culo Procesado {i+1} ---\")\n",
    "        print(f\"TÃ­tulo: {article['title'][:80]}...\")\n",
    "        print(f\"CategorÃ­a: {article.get('category', 'N/A')}\")\n",
    "        print(f\"Fuente: {article['source']}\")\n",
    "        print(f\"Tipo de contenido: {article['content_type']}\")\n",
    "        print(f\"Palabras: {article['word_count']}\")\n",
    "        if isinstance(article['processed_tokens'], list):\n",
    "            print(f\"Tokens procesados: {' '.join(article['processed_tokens'][:10])}...\")\n",
    "        else:\n",
    "            print(f\"Tokens procesados: {str(article['processed_tokens'])[:50]}...\")\n",
    "    \n",
    "    # Separar artÃ­culos crawleados vs dataset\n",
    "    crawled_count = len([a for a in df_processed.iterrows() if a[1]['source'] != 'kaggle_dataset'])\n",
    "    dataset_count = len([a for a in df_processed.iterrows() if a[1]['source'] == 'kaggle_dataset'])\n",
    "    \n",
    "    print(f\"\\nðŸ” Resumen de fuentes:\")\n",
    "    print(f\"  - ArtÃ­culos crawleados: {crawled_count}\")\n",
    "    print(f\"  - ArtÃ­culos del dataset: {dataset_count}\")\n",
    "    \n",
    "    # Guardar datasets\n",
    "    try:\n",
    "        integrator.save_integrated_dataset(df_processed)\n",
    "        crawler.save_crawled_data(crawled_articles)\n",
    "        \n",
    "        print(\"\\nâœ… Â¡Pipeline completado exitosamente!\")\n",
    "        print(\"\\nArchivos generados:\")\n",
    "        print(\"- integrated_news_dataset.csv (Dataset completo procesado)\")\n",
    "        print(\"- crawled_news.json (Datos crawleados en formato JSON)\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Error guardando archivos: {e}\")\n",
    "        print(\"âœ… Pipeline completado (sin guardar archivos)\")\n",
    "    \n",
    "    return df_processed, stats\n",
    "\n",
    "# Ejecutar el pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    final_dataset, dataset_stats = main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
