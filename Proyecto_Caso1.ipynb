{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed2cf67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pandas feedparser nltk beautifulsoup4 requests numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01538f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ punkt ya está disponible\n",
      "✅ stopwords ya está disponible\n",
      "📥 Descargando wordnet...\n",
      "✅ wordnet descargado correctamente\n",
      "✅ averaged_perceptron_tagger ya está disponible\n",
      "✅ punkt_tab ya está disponible\n",
      "✅ Librerías y recursos NLTK configurados\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger_eng.zip.\n"
     ]
    }
   ],
   "source": [
    "# Sistema Inteligente de Búsqueda y Clasificación de Noticias\n",
    "# Módulos: Crawling, Procesamiento e Integración de Dataset\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import feedparser\n",
    "import json\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tag import pos_tag\n",
    "import string\n",
    "from collections import Counter\n",
    "import time\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Descargar recursos de NLTK necesarios\n",
    "def download_nltk_resources():\n",
    "    \"\"\"Descargar todos los recursos de NLTK necesarios\"\"\"\n",
    "    resources = [\n",
    "        ('tokenizers/punkt', 'punkt'),\n",
    "        ('corpora/stopwords', 'stopwords'),\n",
    "        ('corpora/wordnet', 'wordnet'),\n",
    "        ('taggers/averaged_perceptron_tagger', 'averaged_perceptron_tagger'),\n",
    "        ('tokenizers/punkt_tab', 'punkt_tab')\n",
    "    ]\n",
    "    \n",
    "    for resource_path, resource_name in resources:\n",
    "        try:\n",
    "            nltk.data.find(resource_path)\n",
    "            print(f\"✅ {resource_name} ya está disponible\")\n",
    "        except LookupError:\n",
    "            print(f\"📥 Descargando {resource_name}...\")\n",
    "            try:\n",
    "                nltk.download(resource_name, quiet=True)\n",
    "                print(f\"✅ {resource_name} descargado correctamente\")\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Error descargando {resource_name}: {e}\")\n",
    "\n",
    "# Descargar recursos\n",
    "download_nltk_resources()\n",
    "print(\"✅ Librerías y recursos NLTK configurados\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "755d6cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MÓDULO 1: CRAWLING Y RECOPILACIÓN DE DOCUMENTOS\n",
    "# =============================================================================\n",
    "\n",
    "class NewsCrawler:\n",
    "    \"\"\"Crawler para recopilar noticias de diferentes fuentes\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "        })\n",
    "        self.crawled_articles = []\n",
    "    \n",
    "    def crawl_rss_feed(self, feed_url, max_articles=20):\n",
    "        \"\"\"Crawl noticias desde RSS feeds\"\"\"\n",
    "        try:\n",
    "            print(f\"🔍 Crawling RSS: {feed_url}\")\n",
    "            feed = feedparser.parse(feed_url)\n",
    "            articles = []\n",
    "            \n",
    "            for entry in feed.entries[:max_articles]:\n",
    "                article = {\n",
    "                    'title': getattr(entry, 'title', ''),\n",
    "                    'description': getattr(entry, 'summary', ''),\n",
    "                    'link': getattr(entry, 'link', ''),\n",
    "                    'published': getattr(entry, 'published', ''),\n",
    "                    'source': feed_url,\n",
    "                    'content_type': 'rss'\n",
    "                }\n",
    "                \n",
    "                # Intentar obtener contenido completo\n",
    "                try:\n",
    "                    content = self.extract_article_content(article['link'])\n",
    "                    if content:\n",
    "                        article['full_content'] = content\n",
    "                    else:\n",
    "                        article['full_content'] = article['description']\n",
    "                except:\n",
    "                    article['full_content'] = article['description']\n",
    "                \n",
    "                articles.append(article)\n",
    "                time.sleep(0.5)  # Ser respetuoso con el servidor\n",
    "            \n",
    "            print(f\"✅ Obtenidos {len(articles)} artículos de RSS\")\n",
    "            return articles\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error crawling RSS {feed_url}: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def extract_article_content(self, url):\n",
    "        \"\"\"Extraer contenido del artículo desde URL\"\"\"\n",
    "        try:\n",
    "            response = self.session.get(url, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # Remover scripts y estilos\n",
    "            for script in soup([\"script\", \"style\"]):\n",
    "                script.decompose()\n",
    "            \n",
    "            # Buscar contenido principal\n",
    "            content_selectors = [\n",
    "                'article', '.article-content', '.post-content', \n",
    "                '.entry-content', '.content', 'main', '.story-body'\n",
    "            ]\n",
    "            \n",
    "            content = \"\"\n",
    "            for selector in content_selectors:\n",
    "                elements = soup.select(selector)\n",
    "                if elements:\n",
    "                    content = ' '.join([elem.get_text(strip=True) for elem in elements])\n",
    "                    break\n",
    "            \n",
    "            if not content:\n",
    "                # Fallback: obtener todo el texto\n",
    "                content = soup.get_text(strip=True)\n",
    "            \n",
    "            return content[:5000]  # Limitar longitud\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"⚠️  Error extrayendo contenido de {url}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def crawl_news_websites(self, urls, max_articles_per_site=10):\n",
    "        \"\"\"Crawl noticias desde sitios web específicos\"\"\"\n",
    "        all_articles = []\n",
    "        \n",
    "        for url in urls:\n",
    "            try:\n",
    "                print(f\"🔍 Crawling website: {url}\")\n",
    "                response = self.session.get(url, timeout=10)\n",
    "                response.raise_for_status()\n",
    "                soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                \n",
    "                # Buscar enlaces de artículos\n",
    "                article_links = []\n",
    "                link_selectors = ['a[href*=\"article\"]', 'a[href*=\"news\"]', 'a[href*=\"/20\"]']\n",
    "                \n",
    "                for selector in link_selectors:\n",
    "                    links = soup.select(selector)\n",
    "                    for link in links:\n",
    "                        href = link.get('href')\n",
    "                        if href:\n",
    "                            if href.startswith('/'):\n",
    "                                href = url.rstrip('/') + href\n",
    "                            article_links.append(href)\n",
    "                \n",
    "                # Procesar enlaces únicos\n",
    "                unique_links = list(set(article_links))[:max_articles_per_site]\n",
    "                \n",
    "                for link in unique_links:\n",
    "                    try:\n",
    "                        content = self.extract_article_content(link)\n",
    "                        if content and len(content) > 100:\n",
    "                            article = {\n",
    "                                'title': self.extract_title_from_content(content),\n",
    "                                'description': content[:300] + '...',\n",
    "                                'link': link,\n",
    "                                'published': datetime.now().isoformat(),\n",
    "                                'source': url,\n",
    "                                'content_type': 'web',\n",
    "                                'full_content': content\n",
    "                            }\n",
    "                            all_articles.append(article)\n",
    "                    except Exception as e:\n",
    "                        continue\n",
    "                    \n",
    "                    time.sleep(0.5)\n",
    "                \n",
    "                print(f\"✅ Obtenidos {len([a for a in all_articles if a['source'] == url])} artículos de {url}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error crawling {url}: {e}\")\n",
    "        \n",
    "        return all_articles\n",
    "    \n",
    "    def extract_title_from_content(self, content):\n",
    "        \"\"\"Extraer título desde el contenido\"\"\"\n",
    "        sentences = sent_tokenize(content)\n",
    "        if sentences:\n",
    "            return sentences[0][:100]\n",
    "        return \"Sin título\"\n",
    "    \n",
    "    def save_crawled_data(self, articles, filename='crawled_news.json'):\n",
    "        \"\"\"Guardar datos crawleados\"\"\"\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(articles, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"💾 Datos guardados en {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "536c4a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MÓDULO 2: PROCESAMIENTO Y PREPROCESAMIENTO\n",
    "# =============================================================================\n",
    "\n",
    "class TextPreprocessor:\n",
    "    \"\"\"Clase para preprocesamiento de texto\"\"\"\n",
    "    \n",
    "    def __init__(self, language='english'):\n",
    "        self.language = language\n",
    "        self.stop_words = set(stopwords.words(language))\n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        \n",
    "        # Añadir stopwords personalizadas\n",
    "        custom_stopwords = {\n",
    "            'said', 'says', 'say', 'told', 'news', 'report', 'reports',\n",
    "            'according', 'reuters', 'cnn', 'bbc', 'associated', 'press'\n",
    "        }\n",
    "        self.stop_words.update(custom_stopwords)\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Limpieza básica del texto\"\"\"\n",
    "        if not text:\n",
    "            return \"\"\n",
    "        \n",
    "        # Convertir a minúsculas\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Remover URLs\n",
    "        text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n",
    "        \n",
    "        # Remover emails\n",
    "        text = re.sub(r'\\S+@\\S+', '', text)\n",
    "        \n",
    "        # Remover números y caracteres especiales\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
    "        \n",
    "        # Remover espacios múltiples\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        \n",
    "        return text.strip()\n",
    "    \n",
    "    def tokenize_text(self, text):\n",
    "        \"\"\"Tokenización del texto\"\"\"\n",
    "        try:\n",
    "            tokens = word_tokenize(text)\n",
    "            return tokens\n",
    "        except:\n",
    "            return text.split()\n",
    "    \n",
    "    def remove_stopwords(self, tokens):\n",
    "        \"\"\"Remover stopwords\"\"\"\n",
    "        return [token for token in tokens if token not in self.stop_words and len(token) > 2]\n",
    "    \n",
    "    def stem_tokens(self, tokens):\n",
    "        \"\"\"Aplicar stemming\"\"\"\n",
    "        return [self.stemmer.stem(token) for token in tokens]\n",
    "    \n",
    "    def lemmatize_tokens(self, tokens):\n",
    "        \"\"\"Aplicar lemmatización\"\"\"\n",
    "        return [self.lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    def get_pos_tags(self, tokens):\n",
    "        \"\"\"Obtener etiquetas POS\"\"\"\n",
    "        return pos_tag(tokens)\n",
    "    \n",
    "    def filter_by_pos(self, pos_tags, allowed_pos=['NN', 'NNS', 'NNP', 'NNPS', 'JJ', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']):\n",
    "        \"\"\"Filtrar tokens por POS tags\"\"\"\n",
    "        return [word for word, pos in pos_tags if pos in allowed_pos]\n",
    "    \n",
    "    def preprocess_document(self, text, use_stemming=True, use_pos_filtering=True):\n",
    "        \"\"\"Preprocessamiento completo de un documento\"\"\"\n",
    "        # Limpieza\n",
    "        cleaned_text = self.clean_text(text)\n",
    "        \n",
    "        # Tokenización\n",
    "        tokens = self.tokenize_text(cleaned_text)\n",
    "        \n",
    "        # Remover stopwords\n",
    "        tokens = self.remove_stopwords(tokens)\n",
    "        \n",
    "        # Filtrado por POS\n",
    "        if use_pos_filtering:\n",
    "            pos_tags = self.get_pos_tags(tokens)\n",
    "            tokens = self.filter_by_pos(pos_tags)\n",
    "        \n",
    "        # Stemming o Lemmatización\n",
    "        if use_stemming:\n",
    "            tokens = self.stem_tokens(tokens)\n",
    "        else:\n",
    "            tokens = self.lemmatize_tokens(tokens)\n",
    "        \n",
    "        return tokens\n",
    "    \n",
    "    def extract_features(self, text):\n",
    "        \"\"\"Extraer características del texto\"\"\"\n",
    "        tokens = self.preprocess_document(text)\n",
    "        \n",
    "        features = {\n",
    "            'word_count': len(text.split()),\n",
    "            'char_count': len(text),\n",
    "            'sentence_count': len(sent_tokenize(text)),\n",
    "            'avg_word_length': np.mean([len(word) for word in tokens]) if tokens else 0,\n",
    "            'vocabulary_richness': len(set(tokens)) / len(tokens) if tokens else 0,\n",
    "            'most_common_words': Counter(tokens).most_common(10)\n",
    "        }\n",
    "        \n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4a4ad4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MÓDULO 3: INTEGRACIÓN DEL DATASET\n",
    "# =============================================================================\n",
    "\n",
    "class DatasetIntegrator:\n",
    "    \"\"\"Clase para integrar y procesar datasets\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.preprocessor = TextPreprocessor()\n",
    "        self.integrated_dataset = None\n",
    "    \n",
    "    def load_kaggle_dataset(self, filepath):\n",
    "        \"\"\"Cargar dataset de Kaggle\"\"\"\n",
    "        try:\n",
    "            # Intentar diferentes separadores\n",
    "            for sep in [',', '\\t', ';']:\n",
    "                try:\n",
    "                    df = pd.read_csv(filepath, sep=sep)\n",
    "                    if len(df.columns) > 1:\n",
    "                        break\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            print(f\"✅ Dataset de Kaggle cargado: {df.shape[0]} filas, {df.shape[1]} columnas\")\n",
    "            print(f\"Columnas: {list(df.columns)}\")\n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error cargando dataset de Kaggle: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def process_kaggle_dataset(self, df):\n",
    "        \"\"\"Procesar dataset de Kaggle\"\"\"\n",
    "        processed_articles = []\n",
    "        \n",
    "        # Detectar columnas relevantes\n",
    "        text_columns = []\n",
    "        category_column = None\n",
    "        \n",
    "        for col in df.columns:\n",
    "            col_lower = col.lower()\n",
    "            if any(keyword in col_lower for keyword in ['headline', 'title', 'description', 'text', 'content']):\n",
    "                text_columns.append(col)\n",
    "            elif any(keyword in col_lower for keyword in ['category', 'class', 'label', 'topic']):\n",
    "                category_column = col\n",
    "        \n",
    "        print(f\"📝 Columnas de texto detectadas: {text_columns}\")\n",
    "        print(f\"📂 Columna de categoría detectada: {category_column}\")\n",
    "        \n",
    "        for idx, row in df.iterrows():\n",
    "            # Combinar texto de todas las columnas relevantes\n",
    "            text_content = \"\"\n",
    "            for col in text_columns:\n",
    "                if pd.notna(row[col]):\n",
    "                    text_content += str(row[col]) + \" \"\n",
    "            \n",
    "            if text_content.strip():\n",
    "                article = {\n",
    "                    'title': str(row[text_columns[0]]) if text_columns else f\"Article {idx}\",\n",
    "                    'description': text_content[:300] + \"...\",\n",
    "                    'full_content': text_content,\n",
    "                    'category': str(row[category_column]) if category_column and pd.notna(row[category_column]) else 'Unknown',\n",
    "                    'source': 'kaggle_dataset',\n",
    "                    'content_type': 'dataset',\n",
    "                    'published': datetime.now().isoformat(),\n",
    "                    'link': f\"kaggle_article_{idx}\"\n",
    "                }\n",
    "                processed_articles.append(article)\n",
    "        \n",
    "        print(f\"✅ Procesados {len(processed_articles)} artículos del dataset de Kaggle\")\n",
    "        return processed_articles\n",
    "    \n",
    "    def integrate_datasets(self, crawled_articles=None, kaggle_articles=None):\n",
    "        \"\"\"Integrar todos los datasets\"\"\"\n",
    "        all_articles = []\n",
    "        \n",
    "        if crawled_articles:\n",
    "            all_articles.extend(crawled_articles)\n",
    "            print(f\"➕ Añadidos {len(crawled_articles)} artículos crawleados\")\n",
    "        \n",
    "        if kaggle_articles:\n",
    "            all_articles.extend(kaggle_articles)\n",
    "            print(f\"➕ Añadidos {len(kaggle_articles)} artículos del dataset\")\n",
    "        \n",
    "        # Crear DataFrame integrado\n",
    "        df_integrated = pd.DataFrame(all_articles)\n",
    "        \n",
    "        # Limpieza y normalización\n",
    "        df_integrated = df_integrated.drop_duplicates(subset=['title'], keep='first')\n",
    "        df_integrated = df_integrated.dropna(subset=['full_content'])\n",
    "        df_integrated['processed_at'] = datetime.now().isoformat()\n",
    "        \n",
    "        print(f\"🔄 Dataset integrado: {len(df_integrated)} artículos únicos\")\n",
    "        \n",
    "        self.integrated_dataset = df_integrated\n",
    "        return df_integrated\n",
    "    \n",
    "    def preprocess_integrated_dataset(self, df):\n",
    "        \"\"\"Preprocesar dataset integrado con manejo de errores\"\"\"\n",
    "        print(\"🔧 Iniciando preprocesamiento...\")\n",
    "        \n",
    "        # Preprocesar textos con manejo de errores\n",
    "        processed_tokens = []\n",
    "        text_features = []\n",
    "        \n",
    "        for idx, content in enumerate(df['full_content']):\n",
    "            try:\n",
    "                tokens = self.preprocessor.preprocess_document(content)\n",
    "                features = self.preprocessor.extract_features(content)\n",
    "                processed_tokens.append(tokens)\n",
    "                text_features.append(features)\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Error procesando artículo {idx}: {e}\")\n",
    "                # Fallback básico\n",
    "                basic_tokens = content.lower().split()[:50]  # Primeras 50 palabras\n",
    "                basic_features = {\n",
    "                    'word_count': len(content.split()),\n",
    "                    'char_count': len(content),\n",
    "                    'sentence_count': 1,\n",
    "                    'avg_word_length': 5,\n",
    "                    'vocabulary_richness': 0.5,\n",
    "                    'most_common_words': []\n",
    "                }\n",
    "                processed_tokens.append(basic_tokens)\n",
    "                text_features.append(basic_features)\n",
    "        \n",
    "        df['processed_tokens'] = processed_tokens\n",
    "        df['text_features'] = text_features\n",
    "        \n",
    "        # Crear campos adicionales\n",
    "        df['word_count'] = df['text_features'].apply(lambda x: x['word_count'])\n",
    "        df['sentence_count'] = df['text_features'].apply(lambda x: x['sentence_count'])\n",
    "        df['vocab_richness'] = df['text_features'].apply(lambda x: x['vocabulary_richness'])\n",
    "        \n",
    "        # Limpiar categorías\n",
    "        if 'category' in df.columns:\n",
    "            df['category'] = df['category'].str.lower().str.strip()\n",
    "            df['category'] = df['category'].fillna('unknown')\n",
    "        \n",
    "        print(\"✅ Preprocesamiento completado\")\n",
    "        return df\n",
    "    \n",
    "    def get_dataset_statistics(self, df):\n",
    "        \"\"\"Obtener estadísticas del dataset\"\"\"\n",
    "        stats = {\n",
    "            'total_articles': len(df),\n",
    "            'unique_sources': df['source'].nunique(),\n",
    "            'content_types': df['content_type'].value_counts().to_dict(),\n",
    "            'avg_word_count': df['word_count'].mean(),\n",
    "            'avg_sentence_count': df['sentence_count'].mean(),\n",
    "            'categories': df['category'].value_counts().head(10).to_dict() if 'category' in df.columns else {},\n",
    "            'date_range': {\n",
    "                'earliest': df['published'].min() if 'published' in df.columns else None,\n",
    "                'latest': df['published'].max() if 'published' in df.columns else None\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return stats\n",
    "    \n",
    "    def save_integrated_dataset(self, df, filename='integrated_news_dataset.csv'):\n",
    "        \"\"\"Guardar dataset integrado\"\"\"\n",
    "        # Preparar para guardar (convertir listas a strings)\n",
    "        df_save = df.copy()\n",
    "        df_save['processed_tokens'] = df_save['processed_tokens'].apply(lambda x: ' '.join(x) if isinstance(x, list) else str(x))\n",
    "        df_save['text_features'] = df_save['text_features'].apply(lambda x: json.dumps(x) if isinstance(x, dict) else str(x))\n",
    "        \n",
    "        df_save.to_csv(filename, index=False, encoding='utf-8')\n",
    "        print(f\"💾 Dataset integrado guardado en {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa582ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Iniciando Sistema de Crawling y Procesamiento de Noticias\n",
      "\n",
      "============================================================\n",
      "MÓDULO 1: CRAWLING Y RECOPILACIÓN DE DOCUMENTOS\n",
      "============================================================\n",
      "🔍 Intentando crawlear RSS feeds...\n",
      "🔍 Crawling RSS: http://rss.cnn.com/rss/edition.rss\n",
      "⚠️  Error extrayendo contenido de https://www.cnn.com/collections/intl-trump-040223/: 404 Client Error: Not Found for url: https://edition.cnn.com/collections/intl-trump-040223/\n",
      "⚠️  Error extrayendo contenido de https://www.cnn.com/collections/intl-ukraine-030423/: 404 Client Error: Not Found for url: https://edition.cnn.com/collections/intl-ukraine-030423/\n",
      "✅ Obtenidos 10 artículos de RSS\n",
      "✅ 10 artículos obtenidos de http://rss.cnn.com/rss/edition.rss\n",
      "🔍 Crawling RSS: http://feeds.bbci.co.uk/news/rss.xml\n",
      "✅ Obtenidos 10 artículos de RSS\n",
      "✅ 10 artículos obtenidos de http://feeds.bbci.co.uk/news/rss.xml\n",
      "🔍 Crawling RSS: https://rss.reuters.com/news\n",
      "✅ Obtenidos 0 artículos de RSS\n",
      "⚠️ No se pudieron obtener artículos de https://rss.reuters.com/news\n",
      "🔍 Crawling RSS: https://feeds.npr.org/1001/rss.xml\n",
      "✅ Obtenidos 10 artículos de RSS\n",
      "✅ 10 artículos obtenidos de https://feeds.npr.org/1001/rss.xml\n",
      "🔍 Crawling RSS: https://rss.dw.com/rdf/rss-en-all\n",
      "✅ Obtenidos 10 artículos de RSS\n",
      "✅ 10 artículos obtenidos de https://rss.dw.com/rdf/rss-en-all\n",
      "\n",
      "🌐 Intentando crawlear sitios web...\n",
      "🔍 Crawling website: https://news.ycombinator.com\n",
      "⚠️  Error extrayendo contenido de vote?id=44786962&how=up&goto=news: Invalid URL 'vote?id=44786962&how=up&goto=news': No scheme supplied. Perhaps you meant https://vote?id=44786962&how=up&goto=news?\n",
      "⚠️  Error extrayendo contenido de vote?id=44783155&how=up&goto=news: Invalid URL 'vote?id=44783155&how=up&goto=news': No scheme supplied. Perhaps you meant https://vote?id=44783155&how=up&goto=news?\n",
      "✅ Obtenidos 0 artículos de https://news.ycombinator.com\n",
      "🔍 Crawling website: https://www.reddit.com/r/worldnews/.rss\n",
      "✅ Obtenidos 0 artículos de https://www.reddit.com/r/worldnews/.rss\n",
      "🔍 Crawling website: https://techcrunch.com\n",
      "✅ Obtenidos 2 artículos de https://techcrunch.com\n",
      "✅ 2 artículos obtenidos de sitios web\n",
      "\n",
      "📊 Total de artículos crawleados: 42\n",
      "\n",
      "📰 Ejemplos de artículos crawleados:\n",
      "\n",
      "--- Artículo Crawleado 1 ---\n",
      "Título: Trump pleads not guilty to 34 felony counts...\n",
      "Fuente: http://rss.cnn.com/rss/edition.rss\n",
      "Tipo: rss\n",
      "Descripción: ...\n",
      "\n",
      "--- Artículo Crawleado 2 ---\n",
      "Título: Haberman reveals why Trump attacked judge and his family in speech...\n",
      "Fuente: http://rss.cnn.com/rss/edition.rss\n",
      "Tipo: rss\n",
      "Descripción: CNN political contributor Maggie Haberman explains the reasoning behind Donald Trump's attacks on th...\n",
      "\n",
      "============================================================\n",
      "MÓDULO 2: CARGA DEL DATASET DE KAGGLE\n",
      "============================================================\n",
      "📝 Creando dataset de ejemplo para demostración...\n",
      "📝 Columnas de texto detectadas: ['headline', 'short_description']\n",
      "📂 Columna de categoría detectada: category\n",
      "✅ Procesados 10 artículos del dataset de Kaggle\n",
      "\n",
      "============================================================\n",
      "MÓDULO 3: INTEGRACIÓN DEL DATASET\n",
      "============================================================\n",
      "➕ Añadidos 42 artículos crawleados\n",
      "➕ Añadidos 10 artículos del dataset\n",
      "🔄 Dataset integrado: 52 artículos únicos\n",
      "🔧 Iniciando preprocesamiento...\n",
      "✅ Preprocesamiento completado\n",
      "\n",
      "============================================================\n",
      "RESULTADOS Y ESTADÍSTICAS\n",
      "============================================================\n",
      "📊 Total de artículos procesados: 52\n",
      "🌐 Fuentes únicas: 6\n",
      "📝 Promedio de palabras por artículo: 470.6\n",
      "📄 Promedio de oraciones por artículo: 10.2\n",
      "\n",
      "📂 Distribución por tipo de contenido:\n",
      "  - rss: 40 artículos\n",
      "  - dataset: 10 artículos\n",
      "  - web: 2 artículos\n",
      "\n",
      "🏷️  Top categorías:\n",
      "  - unknown: 42 artículos\n",
      "  - technology: 2 artículos\n",
      "  - environment: 2 artículos\n",
      "  - business: 2 artículos\n",
      "  - science: 2 artículos\n",
      "\n",
      "📰 Ejemplos de artículos procesados:\n",
      "\n",
      "--- Artículo Procesado 1 ---\n",
      "Título: Trump pleads not guilty to 34 felony counts...\n",
      "Categoría: unknown\n",
      "Fuente: http://rss.cnn.com/rss/edition.rss\n",
      "Tipo de contenido: rss\n",
      "Palabras: 743\n",
      "Tokens procesados: cover hereform presidentdonald trumparriv new york citi monday day expect...\n",
      "\n",
      "--- Artículo Procesado 2 ---\n",
      "Título: Haberman reveals why Trump attacked judge and his family in speech...\n",
      "Categoría: unknown\n",
      "Fuente: http://rss.cnn.com/rss/edition.rss\n",
      "Tipo de contenido: rss\n",
      "Palabras: 514\n",
      "Tokens procesados: haberman reveal trump attack judg famili speech politicscnn valu feedback...\n",
      "\n",
      "--- Artículo Procesado 3 ---\n",
      "Título: What to know about the Trump indictment on the eve of his court appearance...\n",
      "Categoría: unknown\n",
      "Fuente: http://rss.cnn.com/rss/edition.rss\n",
      "Tipo de contenido: rss\n",
      "Palabras: 0\n",
      "Tokens procesados: ...\n",
      "\n",
      "🔍 Resumen de fuentes:\n",
      "  - Artículos crawleados: 42\n",
      "  - Artículos del dataset: 10\n",
      "💾 Dataset integrado guardado en integrated_news_dataset.csv\n",
      "💾 Datos guardados en crawled_news.json\n",
      "\n",
      "✅ ¡Pipeline completado exitosamente!\n",
      "\n",
      "Archivos generados:\n",
      "- integrated_news_dataset.csv (Dataset completo procesado)\n",
      "- crawled_news.json (Datos crawleados en formato JSON)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# EJECUCIÓN PRINCIPAL\n",
    "# =============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Función principal que ejecuta todo el pipeline\"\"\"\n",
    "    print(\"🚀 Iniciando Sistema de Crawling y Procesamiento de Noticias\\n\")\n",
    "    \n",
    "    # Inicializar componentes\n",
    "    crawler = NewsCrawler()\n",
    "    integrator = DatasetIntegrator()\n",
    "    \n",
    "    # 1. CRAWLING DE NOTICIAS\n",
    "    print(\"=\" * 60)\n",
    "    print(\"MÓDULO 1: CRAWLING Y RECOPILACIÓN DE DOCUMENTOS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    crawled_articles = []\n",
    "    \n",
    "    # RSS Feeds populares\n",
    "    rss_feeds = [\n",
    "        'http://rss.cnn.com/rss/edition.rss',\n",
    "        'http://feeds.bbci.co.uk/news/rss.xml',\n",
    "        'https://rss.reuters.com/news',\n",
    "        'https://feeds.npr.org/1001/rss.xml',\n",
    "        'https://rss.dw.com/rdf/rss-en-all'\n",
    "    ]\n",
    "    \n",
    "    print(\"🔍 Intentando crawlear RSS feeds...\")\n",
    "    for feed in rss_feeds:\n",
    "        try:\n",
    "            articles = crawler.crawl_rss_feed(feed, max_articles=10)\n",
    "            if articles:\n",
    "                crawled_articles.extend(articles)\n",
    "                print(f\"✅ {len(articles)} artículos obtenidos de {feed}\")\n",
    "            else:\n",
    "                print(f\"⚠️ No se pudieron obtener artículos de {feed}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error con {feed}: {e}\")\n",
    "    \n",
    "    # Sitios web alternativos\n",
    "    web_sources = [\n",
    "        'https://news.ycombinator.com',\n",
    "        'https://www.reddit.com/r/worldnews/.rss',\n",
    "        'https://techcrunch.com'\n",
    "    ]\n",
    "    \n",
    "    print(\"\\n🌐 Intentando crawlear sitios web...\")\n",
    "    try:\n",
    "        web_articles = crawler.crawl_news_websites(web_sources, max_articles_per_site=2)\n",
    "        if web_articles:\n",
    "            crawled_articles.extend(web_articles)\n",
    "            print(f\"✅ {len(web_articles)} artículos obtenidos de sitios web\")\n",
    "        else:\n",
    "            print(\"⚠️ No se pudieron obtener artículos de sitios web\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error crawleando sitios web: {e}\")\n",
    "    \n",
    "    \n",
    "    print(f\"\\n📊 Total de artículos crawleados: {len(crawled_articles)}\")\n",
    "    \n",
    "    # Mostrar algunos ejemplos de artículos crawleados\n",
    "    if crawled_articles:\n",
    "        print(\"\\n📰 Ejemplos de artículos crawleados:\")\n",
    "        for i, article in enumerate(crawled_articles[:2]):\n",
    "            print(f\"\\n--- Artículo Crawleado {i+1} ---\")\n",
    "            print(f\"Título: {article['title'][:70]}...\")\n",
    "            print(f\"Fuente: {article['source']}\")\n",
    "            print(f\"Tipo: {article['content_type']}\")\n",
    "            print(f\"Descripción: {article['description'][:100]}...\")\n",
    "    \n",
    "    # 2. CARGA DEL DATASET DE KAGGLE\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"MÓDULO 2: CARGA DEL DATASET DE KAGGLE\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    #----------------------------------------------------------------------------------------\n",
    "    # Crear un dataset de ejemplo más robusto\n",
    "    print(\"📝 Creando dataset de ejemplo para demostración...\")\n",
    "    \n",
    "    example_data = {\n",
    "        'headline': [\n",
    "            'Breaking: New AI Technology Revolutionizes Healthcare Diagnosis',\n",
    "            'Climate Change Effects Visible in Arctic Region Show Alarming Trends',\n",
    "            'Stock Market Reaches New Record High Driven by Tech Sector',\n",
    "            'Major Breakthrough in Renewable Energy Research Promises Cost Reduction',\n",
    "            'International Trade Agreements Under Review by Multiple Nations',\n",
    "            'Education Reform Bill Passes Senate Vote with Bipartisan Support',\n",
    "            'Tech Giant Announces New Product Launch with Revolutionary Features',\n",
    "            'Environmental Protection Measures Implemented Across National Parks',\n",
    "            'Economic Recovery Shows Positive Signs in Manufacturing Sector',\n",
    "            'Scientific Discovery Changes Understanding of Quantum Physics'\n",
    "        ],\n",
    "        'category': [\n",
    "            'TECHNOLOGY', 'ENVIRONMENT', 'BUSINESS', 'SCIENCE', 'POLITICS',\n",
    "            'EDUCATION', 'TECHNOLOGY', 'ENVIRONMENT', 'BUSINESS', 'SCIENCE'\n",
    "        ],\n",
    "        'short_description': [\n",
    "            'Revolutionary AI system promises to transform patient care and medical diagnosis through advanced machine learning algorithms that can detect diseases earlier than traditional methods.',\n",
    "            'Scientists document rapid changes in Arctic ice patterns and wildlife behavior due to global warming trends, with implications for sea level rise and climate stability.',\n",
    "            'Markets surge to unprecedented levels driven by technology sector growth and investor confidence in artificial intelligence and cloud computing companies.',\n",
    "            'Researchers develop highly efficient solar panel technology breakthrough that could revolutionize clean energy production and reduce costs by up to 40 percent.',\n",
    "            'Government officials review existing trade partnerships and agreements to improve economic relationships and address supply chain disruptions affecting global commerce.',\n",
    "            'New legislation aims to modernize educational standards nationwide and improve student outcomes through increased funding and curriculum updates.',\n",
    "            'Company unveils innovative consumer technology product line featuring cutting-edge features like advanced AI integration and sustainable materials.',\n",
    "            'New policies protect endangered species and natural habitats through comprehensive conservation efforts and increased enforcement of environmental regulations.',\n",
    "            'Economic indicators suggest sustained growth and job creation across multiple industry sectors, with manufacturing showing particular strength.',\n",
    "            'Quantum physics research reveals new fundamental particles that challenge existing scientific theories and could lead to breakthrough technologies.'\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    df_kaggle = pd.DataFrame(example_data)\n",
    "    kaggle_articles = integrator.process_kaggle_dataset(df_kaggle)\n",
    "    \n",
    "    # 3. INTEGRACIÓN Y PROCESAMIENTO\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"MÓDULO 3: INTEGRACIÓN DEL DATASET\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Integrar datasets\n",
    "    df_integrated = integrator.integrate_datasets(crawled_articles, kaggle_articles)\n",
    "    \n",
    "    # Preprocesar dataset integrado\n",
    "    df_processed = integrator.preprocess_integrated_dataset(df_integrated)\n",
    "    \n",
    "    # Obtener estadísticas\n",
    "    stats = integrator.get_dataset_statistics(df_processed)\n",
    "    \n",
    "    # 4. MOSTRAR RESULTADOS\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"RESULTADOS Y ESTADÍSTICAS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(f\"📊 Total de artículos procesados: {stats['total_articles']}\")\n",
    "    print(f\"🌐 Fuentes únicas: {stats['unique_sources']}\")\n",
    "    print(f\"📝 Promedio de palabras por artículo: {stats['avg_word_count']:.1f}\")\n",
    "    print(f\"📄 Promedio de oraciones por artículo: {stats['avg_sentence_count']:.1f}\")\n",
    "    \n",
    "    print(\"\\n📂 Distribución por tipo de contenido:\")\n",
    "    for content_type, count in stats['content_types'].items():\n",
    "        print(f\"  - {content_type}: {count} artículos\")\n",
    "    \n",
    "    print(\"\\n🏷️  Top categorías:\")\n",
    "    for category, count in list(stats['categories'].items())[:5]:\n",
    "        print(f\"  - {category}: {count} artículos\")\n",
    "    \n",
    "    # Mostrar ejemplos de artículos procesados\n",
    "    print(\"\\n📰 Ejemplos de artículos procesados:\")\n",
    "    for i, (_, article) in enumerate(df_processed.head(3).iterrows()):\n",
    "        print(f\"\\n--- Artículo Procesado {i+1} ---\")\n",
    "        print(f\"Título: {article['title'][:80]}...\")\n",
    "        print(f\"Categoría: {article.get('category', 'N/A')}\")\n",
    "        print(f\"Fuente: {article['source']}\")\n",
    "        print(f\"Tipo de contenido: {article['content_type']}\")\n",
    "        print(f\"Palabras: {article['word_count']}\")\n",
    "        if isinstance(article['processed_tokens'], list):\n",
    "            print(f\"Tokens procesados: {' '.join(article['processed_tokens'][:10])}...\")\n",
    "        else:\n",
    "            print(f\"Tokens procesados: {str(article['processed_tokens'])[:50]}...\")\n",
    "    \n",
    "    # Separar artículos crawleados vs dataset\n",
    "    crawled_count = len([a for a in df_processed.iterrows() if a[1]['source'] != 'kaggle_dataset'])\n",
    "    dataset_count = len([a for a in df_processed.iterrows() if a[1]['source'] == 'kaggle_dataset'])\n",
    "    \n",
    "    print(f\"\\n🔍 Resumen de fuentes:\")\n",
    "    print(f\"  - Artículos crawleados: {crawled_count}\")\n",
    "    print(f\"  - Artículos del dataset: {dataset_count}\")\n",
    "    \n",
    "    # Guardar datasets\n",
    "    try:\n",
    "        integrator.save_integrated_dataset(df_processed)\n",
    "        crawler.save_crawled_data(crawled_articles)\n",
    "        \n",
    "        print(\"\\n✅ ¡Pipeline completado exitosamente!\")\n",
    "        print(\"\\nArchivos generados:\")\n",
    "        print(\"- integrated_news_dataset.csv (Dataset completo procesado)\")\n",
    "        print(\"- crawled_news.json (Datos crawleados en formato JSON)\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error guardando archivos: {e}\")\n",
    "        print(\"✅ Pipeline completado (sin guardar archivos)\")\n",
    "    \n",
    "    return df_processed, stats\n",
    "\n",
    "# Ejecutar el pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    final_dataset, dataset_stats = main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
